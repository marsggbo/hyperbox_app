# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
    - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
    - override /model: darts.yaml
    - override /datamodule: cifar10_datamodule.yaml
    - override /callbacks: default.yaml
    - override /logger: wandb.yaml
    - override /model/scheduler_cfg: CosineAnnealingLR.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345

trainer:
    min_epochs: 1
    max_epochs: 600

datamodule:
    data_dir: ~/datasets/cifar10
    batch_size: 96
    val_split: 0.5
    is_customized: False

model:
    mutator_cfg:
        _target_: hyperbox.mutator.DartsMutator
    optimizer_cfg:
        _target_: torch.optim.SGD
        lr: 0.025
        weight_decay: 0.0003
        momentum: 0.9
    scheduler_cfg:
        _target_: torch.optim.lr_scheduler.CosineAnnealingLR
        T_max: 50
        eta_min: 1e-4
        last_epoch: -1
    network_cfg:
      n_layers: 20
      channels: 36
      auxiliary: 'cifar' # 'imagenet'
      path_drop_prob: 0.2
      mask: null

logger:
  wandb:
    project: reproduce
    name: reproduce_darts
    offline: False

hydra:
  job:
    name: reproduce_darts
